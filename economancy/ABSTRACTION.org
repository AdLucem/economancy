
#+title: The Program at an Abstract Level


LOOP:
    1. for player in players: GameMachine --[State]--> Players
    2. for player in players: Player --[Action]--> GameMachine
    3. GameMachine :: State -> [Action] -> State 

* Game Machine
** Earning Phase

This one has only one direct step

GameMachine  -- (State + earning) --> Players

** Investing Phase

this one I have already implemented

** Attacking Phase 

Step 1:
GameMachine -> [State phrase=Attacking AttackerIndex Nothing] -> Players 

Step 2:
for player in players:
  if playerIndex == AttackerIndex:
    attackingCardIndex <- Player (State ...)
    return (Attack attackingCardIndex)
  else:
    return Noop

Step 3: 
phase' -> Defending AttackerIndex (AttackingCard = AttackingCardIndex)
State -> State (phase = phase')

** Defending Phase

Step 1:
GameMachine -> Players 

Step 2:
for player in players:
  if playerIndex != attackerIndex:
    defendingCardIndex <- Player 
    return (Defend defendingCardIndex)
  else:
    return Noop

Step 3:

let (atkPlayer, atkCard) <- Defending atkPlayer __AttackingCard__
 
let (defPlayer, defCards) <- extract defcards, playerIds from Defend action

fight

return (State | players -> all players with appropriate cards fainted) 

** Buy Phase

Step 1:

GameMachine -- (State phase = Buy) --> Players

Step 2:

[cardToBuy] <- Players 

Step 3:

- step 3.1:
for card in shop:
  if number of players buying the card > number of cards:
    card leaves the shop and no player gets it

- step 3.2:
for player in players:
  if card.price <= player coins:
    Player <- Player (cards += card, coins -= card.price)
  else:
    Player (coins = 0)

return State (players <- [Player]


* MDP


#+BEGIN_SRC haskell
policy :: State -> Action

reward :: State -> Reward 

transition :: State -> Action -> State
#+END_SRC


* MultiAgent to Single Agent: We Are All Player 0

Although this game is technically a multi-agent system, we'll very crudely approximate a single-agent system by making every other player a part of the environment.

So while from the game system's perspective, it takes a set/unordered list of actions:

#+BEGIN_SRC haskell
gameMachine :: State -> [Action] -> State
#+END_SRC

From the player's perspective, the **transition function** takes only the player's own action.

We hack this by:

(a) Defining a step function that takes a list of `policies`, one for each player - so every player can provide their own policy function to the MDP

(b) Using the above step function to define a transition function- **instantiated for a specific set of policies**- that simply takes a `State` and returns another state (the actions are implicitly provided by the `agent`s.)
 
#+BEGIN_SRC haskell
step :: State -> [Agent] -> State 

-- | Instantiated transition function
instanTransition :: State -> State
instanTransition = step somePolicies 
#+END_SRC

and (c): **While training, we always assume that we are training the policy for player 0**, no matter how many players are in the list.

* MCTS

Monte Carlo tree search constructs a =Tree= where a node is a (State, Action, Value) tuple. We use the =Data.Tree= library for this.

For this particular implementation, we take Value as number of wins + number of draws / number of playouts.

#+BEGIN_SRC haskell
import Data.Tree 
 
data TreeNode = TreeNode {state  :: State,
                          action :: Action,
                          value  :: (Int, Int)}

type MCTree := Tree TreeNode
#+END_SRC 

** Step 1: Selection

Start from root =R= and pick a leaf node =L= via one of three methods:

- Via BFS
- Randomly
- Pick best (most promising) child node out of all child nodes

Here, we demonstrate finding a leaf node in a treenode using some given function =heuristic= (that takes a list of nodes and selects one particular node and returns that). We use a random function in the =heuristic=. 

#+BEGIN_SRC haskell
isLeaf :: MCTree -> Bool
isLeaf (Node _ []) = True 
isLeaf _ = False

selection :: (StdGen ->[MCTree] -> (MCTree, StdGen))
          -> StdGen -> MCTree -> MCTree
selection heuristic randomGen root =
  case (isLeaf root) of
    True -> root
    False -> let
      (bestChild, gen') = heuristic randomGen root.subForest
      in
        selection heuristic gen' bestChild
#+END_SRC

** Step 2: Expansion And Backpropagate

Generate all child nodes of =L= and link them to =L=. Randomly follow one child node.

*** Step 2.1: Generating Search Space

#+BEGIN_SRC haskell
type SearchSpace = [Action]
#+END_SRC

For each state, we need to generate the set of valid moves from that state.

#+BEGIN_SRC haskell
Earning := Noop 

Investing := 
  Invest x; where x <= player.coins

Attacking AttackerIndex _ :=
  if (AttackerIndex == state.playerIndex)
  then Attack x
      where
         (x belongsTo player.cardSet AND
          attack x \= Nothing)
  else Noop

Defending AttackerIndex AttackingCard :=
  if (AttackerIndex == state.playerIndex)
  then Noop 
  else Defend x
    where 
      -- note: we can put another condition here where if
      -- possible, defend x > attack attackingCard
      (x belongsTo player.cardSet AND
       defend x \= nothing)

Buying :=
  if (player.coins > 0) 
  then Buy (Just x)
    where
      (x in state.shop AND
       cost x <= player.coins)
  else Buy Nothing

End _ := Noop 
#+END_SRC

We take the State at the given node and generate a =search space= from it.

#+BEGIN_SRC haskell
genSearchSpace :: TreeNode -> SearchSpace 
genSearchSpace (TreeNode (state _ _)) = getAllValidMoves state 
#+END_SRC

We spin off each element of the search space into its own node and attach it to current node.

#+BEGIN_SRC haskell
expandLeaf :: MCTree -> MCTree 
expandLeaf (Node node children) = 
  let
    searchSpace = genSearchSpace node
    mkChildren = map (\ac -> TreeNode (transition node.state ac) ac (0, 0)) searchSpace   
  in
    Node node mkChildren
#+END_SRC

*** Step 2.2: Randomly Sample from Search Space

We now randomly sample from the children of the given node to get a child node =C=.

#+BEGIN_SRC haskell
randomChild :: MCTree -> MCTree 
randomChild (Node node children) = [random logic]
#+END_SRC

*** Step 2.3: Complete one full playout from =C=


We assume we have a `trajectory :: [State] -> Win | Lose | Draw` function, that takes an initial state and plays out an entire game from that point, and then returns the final result.

#+BEGIN_SRC haskell

-- get state from MCTree node
state :: MCTree -> State

playout :: MCTree -> GameResult
playout MCTree = trajectory [(state MCTree)]  
#+END_SRC

Implementation details are slightly different- remember, because we have multiple functions, our `trajectory` function returns a `Win | Lose | Draw` for every player. And we only take the result for `player 0`, so:
 
#+BEGIN_SRC haskell
playout :: MCTree -> GameResult
playout MCTree = head $ trajectory [(state MCTree)]
#+END_SRC

** Step 3:

Once end state is reached, update =C=, then backpropagate (update values) all the way from =C= to root =R=.

We actually integrate this with step 2- we have a single function to both expand and backpropagate.

Assume we have a function `heuristic` that, given a list of nodes, will return the best node.

#+BEGIN_SRC  haskell
heuristic :: [Node] -> Node
#+END_SRC

#+BEGIN_SRC haskell 

expandBackprop :: MCTree -> MCTree
expandBackprop (Node node children) = 
    case (isChild node) of
      True -> let 
                result = playout (Node node children)
		toVal x = if (x == Lose) then 0 else 1
                value' = (((fst node.value) + (toVal x)), ((snd node.value) + 1))
              in
	        Node (TreeNode node.state node.action value') []
      False -> let
                 -- recurse this function over all children
                 children' = map expandBackprop children
                 -- collect total wins/playouts from all children
                 totalWins = [(fst c.value) | c <- children']
                 totalPlayouts = [(snd c.value) | c <- children']
                 value' = (totalWins, totalPlayouts)
               in
                 Node (node.state node.action value') children'
#+END_SRC



